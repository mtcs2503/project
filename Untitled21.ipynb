{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 1. Download and Preprocess Data\n",
        "def download_data():\n",
        "    url = \"https://mattmahoney.net/dc/text8.zip\"\n",
        "    filename = \"text8.zip\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(\"Downloading text8 dataset...\")\n",
        "        r = requests.get(url)\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "        data = f.read(f.namelist()[0]).decode('utf-8').split()\n",
        "    return data\n",
        "\n",
        "def build_vocab(words, vocab_size=10000):\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
        "    word2idx = {word: i for i, (word, _) in enumerate(count)}\n",
        "\n",
        "    data = []\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in word2idx:\n",
        "            index = word2idx[word]\n",
        "        else:\n",
        "            index = 0  # UNK\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    idx2word = {i: word for word, i in word2idx.items()}\n",
        "    return data, count, word2idx, idx2word\n",
        "\n",
        "# 2. Dataset and Negative Sampling\n",
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, data, word2idx, count, window_size=5, num_neg_samples=5):\n",
        "        self.data = torch.LongTensor(data)\n",
        "        self.window_size = window_size\n",
        "        self.num_neg_samples = num_neg_samples\n",
        "\n",
        "        # Calculate frequencies for negative sampling (P(w)^0.75)\n",
        "        word_counts = np.array([c[1] for c in count])\n",
        "        freqs = word_counts / np.sum(word_counts)\n",
        "        self.neg_weights = torch.from_numpy(freqs ** 0.75)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Target word\n",
        "        center_word = self.data[idx]\n",
        "\n",
        "        # Context window indices\n",
        "        start = max(0, idx - self.window_size)\n",
        "        end = min(len(self.data), idx + self.window_size + 1)\n",
        "\n",
        "        # Pick a random context word from the window\n",
        "        context_idx = random.choice([i for i in range(start, end) if i != idx])\n",
        "        context_word = self.data[context_idx]\n",
        "\n",
        "        # Negative samples\n",
        "        neg_samples = torch.multinomial(self.neg_weights, self.num_neg_samples, replacement=True)\n",
        "\n",
        "        return center_word, context_word, neg_samples\n",
        "\n",
        "# 3. The SGNS Model\n",
        "class SkipGramNeg(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dimension):\n",
        "        super(SkipGramNeg, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "\n",
        "        # Target word embeddings (u)\n",
        "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension)\n",
        "        # Context word embeddings (v)\n",
        "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension)\n",
        "\n",
        "        # Initialization\n",
        "        initrange = 0.5 / emb_dimension\n",
        "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.fill_(0) # Fixed: Changed .constant_(0) to .fill_(0)\n",
        "\n",
        "    def forward(self, center_words, context_words, negative_samples):\n",
        "        # center_words: [batch_size]\n",
        "        # context_words: [batch_size]\n",
        "        # negative_samples: [batch_size, num_neg_samples]\n",
        "\n",
        "        emb_u = self.u_embeddings(center_words)    # [batch_size, emb_dim]\n",
        "        emb_v = self.v_embeddings(context_words)   # [batch_size, emb_dim]\n",
        "        emb_neg = self.v_embeddings(negative_samples) # [batch_size, num_neg, emb_dim]\n",
        "\n",
        "        # Positive loss: log(sigmoid(u dot v))\n",
        "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
        "        pos_loss = torch.log(torch.sigmoid(score))\n",
        "\n",
        "        # Negative loss: sum(log(sigmoid(-u dot v_neg)))\n",
        "        # bmm = Batch Matrix Multiplication\n",
        "        neg_score = torch.bmm(emb_neg, emb_u.unsqueeze(2)).squeeze(2) # [batch_size, num_neg]\n",
        "        neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score)), dim=1)\n",
        "\n",
        "        return -(pos_loss + neg_loss).mean()\n",
        "\n",
        "# 4. Training Loop\n",
        "def train():\n",
        "    # Hyperparameters\n",
        "    VOCAB_SIZE = 20000\n",
        "    EMBED_DIM = 100\n",
        "    WINDOW_SIZE = 5\n",
        "    NEG_SAMPLES = 5\n",
        "    BATCH_SIZE = 1024\n",
        "    EPOCHS = 1\n",
        "    LR = 0.001\n",
        "\n",
        "    # Prepare data\n",
        "    raw_words = download_data()\n",
        "    data, count, word2idx, idx2word = build_vocab(raw_words, VOCAB_SIZE)\n",
        "    dataset = SkipGramDataset(data, word2idx, count, WINDOW_SIZE, NEG_SAMPLES)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SkipGramNeg(VOCAB_SIZE, EMBED_DIM).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    print(f\"Starting training on {device}...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for i, (center, context, negs) in enumerate(dataloader):\n",
        "            center, context, negs = center.to(device), context.to(device), negs.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(center, context, negs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Save embeddings\n",
        "    embeddings = model.u_embeddings.weight.data.cpu().numpy()\n",
        "    return embeddings, word2idx, idx2word\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    embeddings, word2idx, idx2word = train()\n",
        "\n",
        "    # Test Similarity\n",
        "    def get_similarity(word, embeddings, word2idx, idx2word, top_k=5):\n",
        "        if word not in word2idx:\n",
        "            return \"Word not in vocab\"\n",
        "        idx = word2idx[word]\n",
        "        vector = embeddings[idx]\n",
        "        # Cosine similarity\n",
        "        sim = np.dot(embeddings, vector) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(vector))\n",
        "        nearest = (-sim).argsort()[1:top_k+1]\n",
        "        return [idx2word[i] for i in nearest]\n",
        "\n",
        "    test_word = \"queen\"\n",
        "    print(f\"Words nearest to '{test_word}': {get_similarity(test_word, embeddings, word2idx, idx2word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXyuMAUikzP8",
        "outputId": "b30d46e8-d668-4bed-9d22-10a272b7e2f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 0, Loss: 4.1589\n",
            "Epoch 0, Batch 1000, Loss: 2.5113\n",
            "Epoch 0, Batch 2000, Loss: 2.4690\n",
            "Epoch 0, Batch 3000, Loss: 2.4006\n",
            "Epoch 0, Batch 4000, Loss: 2.4069\n",
            "Epoch 0, Batch 5000, Loss: 2.4166\n",
            "Epoch 0, Batch 6000, Loss: 2.4115\n",
            "Epoch 0, Batch 7000, Loss: 2.3935\n",
            "Epoch 0, Batch 8000, Loss: 2.4176\n",
            "Epoch 0, Batch 9000, Loss: 2.3763\n",
            "Epoch 0, Batch 10000, Loss: 2.3969\n",
            "Epoch 0, Batch 11000, Loss: 2.3967\n",
            "Epoch 0, Batch 12000, Loss: 2.3943\n",
            "Epoch 0, Batch 13000, Loss: 2.3734\n",
            "Epoch 0, Batch 14000, Loss: 2.3536\n",
            "Epoch 0, Batch 15000, Loss: 2.3633\n",
            "Epoch 0, Batch 16000, Loss: 2.3333\n",
            "Words nearest to 'queen': ['constantine', 'patriarch', 'grandson', 'vii', 'sigismund']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1. SETUP: We assume 'embeddings' and 'word2idx' are from the PyTorch model\n",
        "# trained in the previous step. For this script, we'll wrap them for easy access.\n",
        "class MyModelWrapper:\n",
        "    def __init__(self, embeddings, word2idx, idx2word):\n",
        "        self.embeddings = embeddings\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        if word in self.word2idx:\n",
        "            return self.embeddings[self.word2idx[word]]\n",
        "        return None\n",
        "\n",
        "    def similarity(self, w1, w2):\n",
        "        v1 = self.get_vector(w1)\n",
        "        v2 = self.get_vector(w2)\n",
        "        if v1 is None or v2 is None: return 0\n",
        "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "# 2. TRAIN GENSIM MODEL (on the same text8 dataset)\n",
        "print(\"Loading text8 and training Gensim model...\")\n",
        "dataset = api.load('text8')\n",
        "# sg=1 (Skip-gram), vector_size=100 to match our manual implementation\n",
        "gensim_model = Word2Vec(sentences=dataset, vector_size=100, window=5, min_count=5, sg=1, workers=4)\n",
        "gensim_wv = gensim_model.wv\n",
        "\n",
        "# 3. COMPARE MODELS\n",
        "my_model = MyModelWrapper(embeddings, word2idx, idx2word) # 'embeddings' from previous step\n",
        "\n",
        "test_pairs = [\n",
        "    (\"king\", \"queen\"),\n",
        "    (\"man\", \"woman\"),\n",
        "    (\"france\", \"paris\"),\n",
        "    (\"car\", \"engine\"),\n",
        "    (\"dog\", \"cat\"),\n",
        "    (\"computer\", \"keyboard\")\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Word Pair':<20} | {'Manual Sim':<12} | {'Gensim Sim':<12} | {'Diff'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for w1, w2 in test_pairs:\n",
        "    try:\n",
        "        sim_mine = my_model.similarity(w1, w2)\n",
        "        sim_gensim = gensim_wv.similarity(w1, w2)\n",
        "        print(f\"{w1 + '-' + w2:<20} | {sim_mine:12.4f} | {sim_gensim:12.4f} | {abs(sim_mine-sim_gensim):.4f}\")\n",
        "    except KeyError:\n",
        "        print(f\"{w1}-{w2} not in vocab\")\n",
        "\n",
        "# 4. COMPARE TOP-K NEIGHBORS\n",
        "target_word = \"queen\"\n",
        "print(f\"\\nTop 5 Nearest Neighbors for '{target_word}':\")\n",
        "\n",
        "# Manual Neighbors\n",
        "idx = word2idx[target_word]\n",
        "vec = embeddings[idx]\n",
        "sims = np.dot(embeddings, vec) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(vec))\n",
        "manual_neighbors = [idx2word[i] for i in sims.argsort()[::-1][1:6]]\n",
        "\n",
        "# Gensim Neighbors\n",
        "gensim_neighbors = [w for w, s in gensim_wv.most_similar(target_word, topn=5)]\n",
        "\n",
        "print(f\"Manual implementation: {manual_neighbors}\")\n",
        "print(f\"Gensim implementation: {gensim_neighbors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-vAfHrHlEG1",
        "outputId": "e6c7462a-34cc-4f14-f9f9-a70ac33ae35c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading text8 and training Gensim model...\n",
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n",
            "\n",
            "Word Pair            | Manual Sim   | Gensim Sim   | Diff\n",
            "-----------------------------------------------------------------\n",
            "king-queen           |       0.9050 |       0.7220 | 0.1830\n",
            "man-woman            |       0.8049 |       0.7242 | 0.0807\n",
            "france-paris         |       0.7530 |       0.7221 | 0.0309\n",
            "car-engine           |       0.6786 |       0.6024 | 0.0762\n",
            "dog-cat              |       0.8873 |       0.6729 | 0.2144\n",
            "computer-keyboard    |       0.7813 |       0.5267 | 0.2546\n",
            "\n",
            "Top 5 Nearest Neighbors for 'queen':\n",
            "Manual implementation: ['constantine', 'patriarch', 'grandson', 'vii', 'sigismund']\n",
            "Gensim implementation: ['elizabeth', 'consort', 'regnant', 'prince', 'highness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_analogy(a, b, c, embeddings, word2idx, idx2word, top_k=1):\n",
        "    for word in [a, b, c]:\n",
        "        if word not in word2idx:\n",
        "            return f\"'{word}' not in vocabulary.\"\n",
        "\n",
        "    # Vector arithmetic: v_b - v_a + v_c\n",
        "    vec_a = embeddings[word2idx[a]]\n",
        "    vec_b = embeddings[word2idx[b]]\n",
        "    vec_c = embeddings[word2idx[c]]\n",
        "    target_vec = vec_b - vec_a + vec_c\n",
        "\n",
        "    # Calculate cosine similarity with all vectors\n",
        "    norm_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    norm_target = target_vec / np.linalg.norm(target_vec)\n",
        "\n",
        "    similarities = np.dot(norm_embeddings, norm_target)\n",
        "\n",
        "    # Get top results, excluding the input words\n",
        "    sorted_indices = np.argsort(similarities)[::-1]\n",
        "    results = []\n",
        "    for idx in sorted_indices:\n",
        "        word = idx2word[idx]\n",
        "        if word not in [a, b, c]:\n",
        "            results.append((word, similarities[idx]))\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test the analogy\n",
        "print(f\"man : king :: woman : ?\", solve_analogy(\"man\", \"king\", \"woman\", embeddings, word2idx, idx2word))\n",
        "print(f\"france : paris :: germany : ?\", solve_analogy(\"france\", \"paris\", \"germany\", embeddings, word2idx, idx2word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xRhntWHlQ2B",
        "outputId": "aee94e34-8b41-457d-c6d8-3c0bc74da924"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man : king :: woman : ? [('afonso', np.float32(0.8578553))]\n",
            "france : paris :: germany : ? [('zurich', np.float32(0.8630364))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_gender_bias(neutral_words, embeddings, word2idx):\n",
        "    # 1. Define the gender direction\n",
        "    def get_diff(w1, w2):\n",
        "        return embeddings[word2idx[w1]] - embeddings[word2idx[w2]]\n",
        "\n",
        "    # Average several pairs to get a more stable \"gender axis\"\n",
        "    gender_axis = (get_diff('man', 'woman') + get_diff('he', 'she')) / 2.0\n",
        "    gender_axis /= np.linalg.norm(gender_axis) # Normalize\n",
        "\n",
        "    bias_scores = []\n",
        "    for word in neutral_words:\n",
        "        if word in word2idx:\n",
        "            vec = embeddings[word2idx[word]]\n",
        "            vec /= np.linalg.norm(vec)\n",
        "            # Projection onto gender axis\n",
        "            score = np.dot(vec, gender_axis)\n",
        "            bias_scores.append((word, score))\n",
        "\n",
        "    # Sort by bias score (most masculine to most feminine)\n",
        "    return sorted(bias_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "occupations = [\"doctor\", \"nurse\", \"engineer\", \"teacher\", \"homemaker\", \"scientist\", \"boss\", \"secretary\"]\n",
        "results = detect_gender_bias(occupations, embeddings, word2idx)\n",
        "\n",
        "print(f\"{'Word':<12} | {'Gender Bias Score'}\")\n",
        "print(\"-\" * 30)\n",
        "for word, score in results:\n",
        "    print(f\"{word:<12} | {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE_V6k2AlXaO",
        "outputId": "28c62934-8477-4bac-a936-acf9576a629a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word         | Gender Bias Score\n",
            "------------------------------\n",
            "scientist    | 0.0995\n",
            "secretary    | 0.0746\n",
            "engineer     | 0.0539\n",
            "doctor       | 0.0354\n",
            "teacher      | -0.0030\n",
            "nurse        | -0.0990\n",
            "boss         | -0.1281\n"
          ]
        }
      ]
    }
  ]
}